{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvjGt589SAUu5IUXHPGxgI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjibsinha/Machine-Learning-Primer/blob/main/Generating_Text_with_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tJdSEyV4Hgsf"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Import numpy for numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# Import layers we'll need for our text model\n",
        "from keras.layers import TextVectorization, Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our training text\n",
        "text = \"\"\"\n",
        "Shall I compare thee to a summer's day?\n",
        "Thou art more lovely and more temperate:\n",
        "Rough winds do shake the darling buds of May,\n",
        "And summer's lease hath all too short a date:\n",
        "\"\"\"\n",
        "\n",
        "# Create a vocabulary of all unique characters in the text\n",
        "vocab = sorted(list(set(text)))\n",
        "print(f\"Vocabulary: {''.join(vocab)}\")\n",
        "print(f\"Number of unique characters: {len(vocab)}\")\n",
        "\n",
        "# Create a mapping from characters to integers (and vice-versa)\n",
        "char_to_int = {char: i for i, char in enumerate(vocab)}\n",
        "int_to_char = {i: char for i, char in enumerate(vocab)}\n",
        "\n",
        "# Convert our entire text to integers\n",
        "encoded_text = np.array([char_to_int[char] for char in text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBtwoFopIew9",
        "outputId": "b4e98511-3d9f-49bb-8989-d9f0c2c4a972"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: \n",
            " ',:?AIMRSTabcdefghiklmnoprstuvwy\n",
            "Number of unique characters: 34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences and targets\n",
        "seq_length = 20\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(encoded_text) - seq_length):\n",
        "    sequences.append(encoded_text[i:i+seq_length])\n",
        "    targets.append(encoded_text[i+seq_length])\n",
        "\n",
        "X = np.array(sequences)\n",
        "y = np.array(targets)"
      ],
      "metadata": {
        "id": "kdKOvs25IvCA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = keras.Sequential([\n",
        "    # Embedding layer\n",
        "    Embedding(input_dim=len(vocab), output_dim=256, input_length=seq_length),\n",
        "\n",
        "    # LSTM layer\n",
        "    LSTM(512),\n",
        "\n",
        "    # Output layer (one neuron for each character in our vocabulary)\n",
        "    Dense(len(vocab), activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLNH_EqmJB4s",
        "outputId": "2460fa65-45c0-4c2a-b480-9ed1e5175465"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "# Note: For good results, text models often need 50-100 epochs or more.\n",
        "# We'll use 50 here for a demonstration.\n",
        "history = model.fit(X, y, epochs=50, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Wy5V81JX1r",
        "outputId": "b799a8d5-312b-4b25-bba2-91f4471c7ed8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 - 4s - 791ms/step - accuracy: 0.1234 - loss: 3.4753\n",
            "Epoch 2/50\n",
            "5/5 - 1s - 257ms/step - accuracy: 0.1688 - loss: 3.3763\n",
            "Epoch 3/50\n",
            "5/5 - 3s - 510ms/step - accuracy: 0.1299 - loss: 3.3038\n",
            "Epoch 4/50\n",
            "5/5 - 3s - 589ms/step - accuracy: 0.1364 - loss: 3.1936\n",
            "Epoch 5/50\n",
            "5/5 - 1s - 269ms/step - accuracy: 0.1234 - loss: 3.0123\n",
            "Epoch 6/50\n",
            "5/5 - 1s - 278ms/step - accuracy: 0.1688 - loss: 2.9966\n",
            "Epoch 7/50\n",
            "5/5 - 3s - 504ms/step - accuracy: 0.1688 - loss: 2.9470\n",
            "Epoch 8/50\n",
            "5/5 - 1s - 262ms/step - accuracy: 0.1883 - loss: 2.8843\n",
            "Epoch 9/50\n",
            "5/5 - 2s - 326ms/step - accuracy: 0.2273 - loss: 2.9131\n",
            "Epoch 10/50\n",
            "5/5 - 3s - 578ms/step - accuracy: 0.1883 - loss: 2.8440\n",
            "Epoch 11/50\n",
            "5/5 - 2s - 392ms/step - accuracy: 0.1818 - loss: 2.8053\n",
            "Epoch 12/50\n",
            "5/5 - 2s - 487ms/step - accuracy: 0.1883 - loss: 2.7603\n",
            "Epoch 13/50\n",
            "5/5 - 3s - 517ms/step - accuracy: 0.2403 - loss: 2.6846\n",
            "Epoch 14/50\n",
            "5/5 - 3s - 504ms/step - accuracy: 0.2597 - loss: 2.6117\n",
            "Epoch 15/50\n",
            "5/5 - 1s - 271ms/step - accuracy: 0.2662 - loss: 2.5059\n",
            "Epoch 16/50\n",
            "5/5 - 3s - 558ms/step - accuracy: 0.3247 - loss: 2.3729\n",
            "Epoch 17/50\n",
            "5/5 - 2s - 456ms/step - accuracy: 0.3247 - loss: 2.2731\n",
            "Epoch 18/50\n",
            "5/5 - 3s - 512ms/step - accuracy: 0.3377 - loss: 2.0928\n",
            "Epoch 19/50\n",
            "5/5 - 2s - 499ms/step - accuracy: 0.4091 - loss: 1.9148\n",
            "Epoch 20/50\n",
            "5/5 - 3s - 624ms/step - accuracy: 0.5260 - loss: 1.7335\n",
            "Epoch 21/50\n",
            "5/5 - 2s - 450ms/step - accuracy: 0.5325 - loss: 1.5938\n",
            "Epoch 22/50\n",
            "5/5 - 1s - 267ms/step - accuracy: 0.5909 - loss: 1.3602\n",
            "Epoch 23/50\n",
            "5/5 - 1s - 265ms/step - accuracy: 0.6494 - loss: 1.2076\n",
            "Epoch 24/50\n",
            "5/5 - 3s - 505ms/step - accuracy: 0.6883 - loss: 1.0734\n",
            "Epoch 25/50\n",
            "5/5 - 3s - 512ms/step - accuracy: 0.7078 - loss: 0.9629\n",
            "Epoch 26/50\n",
            "5/5 - 3s - 657ms/step - accuracy: 0.8182 - loss: 0.7923\n",
            "Epoch 27/50\n",
            "5/5 - 2s - 389ms/step - accuracy: 0.8377 - loss: 0.7006\n",
            "Epoch 28/50\n",
            "5/5 - 1s - 262ms/step - accuracy: 0.9286 - loss: 0.4876\n",
            "Epoch 29/50\n",
            "5/5 - 1s - 265ms/step - accuracy: 0.9286 - loss: 0.4082\n",
            "Epoch 30/50\n",
            "5/5 - 3s - 511ms/step - accuracy: 0.9805 - loss: 0.3105\n",
            "Epoch 31/50\n",
            "5/5 - 1s - 263ms/step - accuracy: 0.9805 - loss: 0.2598\n",
            "Epoch 32/50\n",
            "5/5 - 1s - 267ms/step - accuracy: 0.9870 - loss: 0.2066\n",
            "Epoch 33/50\n",
            "5/5 - 1s - 298ms/step - accuracy: 0.9935 - loss: 0.1775\n",
            "Epoch 34/50\n",
            "5/5 - 2s - 418ms/step - accuracy: 0.9935 - loss: 0.1391\n",
            "Epoch 35/50\n",
            "5/5 - 2s - 313ms/step - accuracy: 1.0000 - loss: 0.0975\n",
            "Epoch 36/50\n",
            "5/5 - 2s - 467ms/step - accuracy: 1.0000 - loss: 0.0718\n",
            "Epoch 37/50\n",
            "5/5 - 1s - 264ms/step - accuracy: 1.0000 - loss: 0.0602\n",
            "Epoch 38/50\n",
            "5/5 - 3s - 507ms/step - accuracy: 1.0000 - loss: 0.0443\n",
            "Epoch 39/50\n",
            "5/5 - 1s - 264ms/step - accuracy: 1.0000 - loss: 0.0388\n",
            "Epoch 40/50\n",
            "5/5 - 3s - 636ms/step - accuracy: 1.0000 - loss: 0.0327\n",
            "Epoch 41/50\n",
            "5/5 - 2s - 365ms/step - accuracy: 1.0000 - loss: 0.0281\n",
            "Epoch 42/50\n",
            "5/5 - 1s - 261ms/step - accuracy: 1.0000 - loss: 0.0256\n",
            "Epoch 43/50\n",
            "5/5 - 3s - 515ms/step - accuracy: 1.0000 - loss: 0.0223\n",
            "Epoch 44/50\n",
            "5/5 - 3s - 502ms/step - accuracy: 1.0000 - loss: 0.0209\n",
            "Epoch 45/50\n",
            "5/5 - 3s - 517ms/step - accuracy: 1.0000 - loss: 0.0190\n",
            "Epoch 46/50\n",
            "5/5 - 2s - 339ms/step - accuracy: 1.0000 - loss: 0.0176\n",
            "Epoch 47/50\n",
            "5/5 - 2s - 416ms/step - accuracy: 1.0000 - loss: 0.0162\n",
            "Epoch 48/50\n",
            "5/5 - 1s - 260ms/step - accuracy: 1.0000 - loss: 0.0152\n",
            "Epoch 49/50\n",
            "5/5 - 3s - 511ms/step - accuracy: 1.0000 - loss: 0.0144\n",
            "Epoch 50/50\n",
            "5/5 - 1s - 263ms/step - accuracy: 1.0000 - loss: 0.0133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(model, start_string, num_generate=200):\n",
        "    # Convert the start string to integers\n",
        "    input_eval = [char_to_int[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # Remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Use a categorical distribution to predict the character returned by the model\n",
        "        predicted_id = tf.random.categorical(tf.expand_dims(predictions, 0), num_samples=1)[0,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(int_to_char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "# Generate text starting with \"Thou art\"\n",
        "print(generate_text(model, start_string=\"Thou art\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeAIZCtiKS8n",
        "outputId": "ac0905b2-5ce6-43d1-b8ff-3f8f08cef0c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thou artk uAcoyfTo u?gfdbaff,flgMI\n",
            "dRRTcr:gvdT  uTwwlAknyrRkrd'  sfrlMAuhdMofTiSohvs\n",
            "cRoR?bgwv,o?Iy:wdct'Ssh\n",
            "bSkSvv,,itTvr s\n",
            "rpl?T:afsn?M?I'yReefl:wbR?fasniatuvnovyvb,ycl\n",
            "SvMd\n",
            "yScgoR:werlSIuc:h\n",
            "aIrpwhMTo:hifI\n"
          ]
        }
      ]
    }
  ]
}